{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/timicienio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "import re\n",
    "from queue import PriorityQueue\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from service.lyrics_classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualPriorityQueue(PriorityQueue):\n",
    "    def __init__(self, maxPQ=False):\n",
    "        PriorityQueue.__init__(self)\n",
    "        self.reverse = -1 if maxPQ else 1\n",
    "\n",
    "    def put(self, priority, data):\n",
    "        PriorityQueue.put(self, (self.reverse * priority, data))\n",
    "\n",
    "    def get(self, *args, **kwargs):\n",
    "        priority, data = PriorityQueue.get(self, *args, **kwargs)\n",
    "        return self.reverse * priority, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['family', 'subfamily', 'ingredients', 'gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wallstreet</td>\n",
       "      <td>A modern aroma, ideal for urban men and that h...</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L'Eau D'Issey Shade Of Sunrise 2019</td>\n",
       "      <td>Day 1, 5:45 am. \" evokes a joyous, exciting an...</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Opus 1144</td>\n",
       "      <td>Inspired by Gothicism (circa 1144), Opus 1144 ...</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blu Di Roma Uomo</td>\n",
       "      <td>Blu di Roma, a romantic, intense, and fresh fr...</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rem L'Acqua</td>\n",
       "      <td>Celebrate 20 years of Rem with Rem L'Acqua! Im...</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132842</th>\n",
       "      <td>Al Abiq Oud</td>\n",
       "      <td>The addition of precious woods in the base ele...</td>\n",
       "      <td>Oud - Agarwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132843</th>\n",
       "      <td>Inara Oud</td>\n",
       "      <td>Inara Oud was inspired by the woman whose radi...</td>\n",
       "      <td>Oud - Agarwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132844</th>\n",
       "      <td>Ganga</td>\n",
       "      <td>Inspired by the majestic rivers of India, Gang...</td>\n",
       "      <td>Oud - Agarwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132845</th>\n",
       "      <td>Rosamunda</td>\n",
       "      <td>Laboratorio Olfattivo has taken up the challen...</td>\n",
       "      <td>Oud - Agarwood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132846</th>\n",
       "      <td>Oud</td>\n",
       "      <td>This strong and captivating fragrance makes su...</td>\n",
       "      <td>Oud - Agarwood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132847 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name  \\\n",
       "0                                Wallstreet   \n",
       "1       L'Eau D'Issey Shade Of Sunrise 2019   \n",
       "2                                 Opus 1144   \n",
       "3                          Blu Di Roma Uomo   \n",
       "4                               Rem L'Acqua   \n",
       "...                                     ...   \n",
       "132842                          Al Abiq Oud   \n",
       "132843                            Inara Oud   \n",
       "132844                                Ganga   \n",
       "132845                            Rosamunda   \n",
       "132846                                  Oud   \n",
       "\n",
       "                                              description     ingredients  \n",
       "0       A modern aroma, ideal for urban men and that h...           Lemon  \n",
       "1       Day 1, 5:45 am. \" evokes a joyous, exciting an...           Lemon  \n",
       "2       Inspired by Gothicism (circa 1144), Opus 1144 ...           Lemon  \n",
       "3       Blu di Roma, a romantic, intense, and fresh fr...           Lemon  \n",
       "4       Celebrate 20 years of Rem with Rem L'Acqua! Im...           Lemon  \n",
       "...                                                   ...             ...  \n",
       "132842  The addition of precious woods in the base ele...  Oud - Agarwood  \n",
       "132843  Inara Oud was inspired by the woman whose radi...  Oud - Agarwood  \n",
       "132844  Inspired by the majestic rivers of India, Gang...  Oud - Agarwood  \n",
       "132845  Laboratorio Olfattivo has taken up the challen...  Oud - Agarwood  \n",
       "132846  This strong and captivating fragrance makes su...  Oud - Agarwood  \n",
       "\n",
       "[132847 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = features[2]\n",
    "\n",
    "directory = f'../data/{feature}'\n",
    "\n",
    "\n",
    "# List to store all the DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "\n",
    "    # Check if the file is a CSV file\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Read the CSV file and append it to the list of DataFrames\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "        feature_value = os.path.splitext(filename)[0]\n",
    "        df[feature] = feature_value\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# Print the first few rows to verify\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Empty Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin, test_origin = train_test_split(\n",
    "    combined_df, test_size=0.01, stratify=combined_df[feature]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_origin.reset_index(drop=True)\n",
    "test = test_origin.reset_index(drop=True)\n",
    "train[\"index\"] = train.index\n",
    "test[\"index\"] = test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Am Extrait</td>\n",
       "      <td>Her sensual, tender sillage charms and captiva...</td>\n",
       "      <td>Amber</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voile De Jasmin</td>\n",
       "      <td>Bvlgari Voile de Jasmin is a delicate ovocatio...</td>\n",
       "      <td>Musk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overdose Aphrodisiaque</td>\n",
       "      <td>Thirsty for independence, the anti-conformist ...</td>\n",
       "      <td>Incense - Olibanum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Palais Bourbon</td>\n",
       "      <td>A fragrance of contrast and cooperation, Palai...</td>\n",
       "      <td>Benzoin</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Black Onyx</td>\n",
       "      <td>Black Onyx is a refreshing fragrance shrouded ...</td>\n",
       "      <td>Oud - Agarwood</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363565</th>\n",
       "      <td>Skye</td>\n",
       "      <td>A distinctive bouquet made from a stunning ble...</td>\n",
       "      <td>Ylang-ylang</td>\n",
       "      <td>58118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363566</th>\n",
       "      <td>Sincere</td>\n",
       "      <td>This scent is an enrobing expression of profou...</td>\n",
       "      <td>Ylang-ylang</td>\n",
       "      <td>95194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363567</th>\n",
       "      <td>Vol 870 Yul-Cdg</td>\n",
       "      <td>This perfume expresses the beginning of a beau...</td>\n",
       "      <td>Ylang-ylang</td>\n",
       "      <td>39032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363568</th>\n",
       "      <td>Eau De Lune</td>\n",
       "      <td>Laura Mercier's Eau de Lune offers a romantic ...</td>\n",
       "      <td>Ylang-ylang</td>\n",
       "      <td>21106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363569</th>\n",
       "      <td>Diaghilev Parfum</td>\n",
       "      <td>Diaghilev pays homage to the beautiful feminin...</td>\n",
       "      <td>Ylang-ylang</td>\n",
       "      <td>102732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363570 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  \\\n",
       "0                 I Am Extrait   \n",
       "1              Voile De Jasmin   \n",
       "2       Overdose Aphrodisiaque   \n",
       "3               Palais Bourbon   \n",
       "4                   Black Onyx   \n",
       "...                        ...   \n",
       "363565                    Skye   \n",
       "363566                 Sincere   \n",
       "363567         Vol 870 Yul-Cdg   \n",
       "363568             Eau De Lune   \n",
       "363569        Diaghilev Parfum   \n",
       "\n",
       "                                              description         ingredients  \\\n",
       "0       Her sensual, tender sillage charms and captiva...               Amber   \n",
       "1       Bvlgari Voile de Jasmin is a delicate ovocatio...                Musk   \n",
       "2       Thirsty for independence, the anti-conformist ...  Incense - Olibanum   \n",
       "3       A fragrance of contrast and cooperation, Palai...             Benzoin   \n",
       "4       Black Onyx is a refreshing fragrance shrouded ...      Oud - Agarwood   \n",
       "...                                                   ...                 ...   \n",
       "363565  A distinctive bouquet made from a stunning ble...         Ylang-ylang   \n",
       "363566  This scent is an enrobing expression of profou...         Ylang-ylang   \n",
       "363567  This perfume expresses the beginning of a beau...         Ylang-ylang   \n",
       "363568  Laura Mercier's Eau de Lune offers a romantic ...         Ylang-ylang   \n",
       "363569  Diaghilev pays homage to the beautiful feminin...         Ylang-ylang   \n",
       "\n",
       "         index  \n",
       "0            0  \n",
       "1            1  \n",
       "2            2  \n",
       "3            3  \n",
       "4            4  \n",
       "...        ...  \n",
       "363565   58118  \n",
       "363566   95194  \n",
       "363567   39032  \n",
       "363568   21106  \n",
       "363569  102732  \n",
       "\n",
       "[363570 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy='auto', random_state=27)\n",
    "train = oversampler.fit_resample(train, train[feature])[0]\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/timicienio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def text_split(text):\n",
    "    text = re.split(\"[^a-zA-Z]+\", text)\n",
    "    text = [x for x in text if x]\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # turn into lower case\n",
    "    text = text.lower()\n",
    "    # tokenize\n",
    "    words = text_split(text)\n",
    "\n",
    "    # words =  [''.join(filter(str.isalnum, word)) for word in words]\n",
    "    words = [word for word in words if word != \"\"]\n",
    "\n",
    "    # PorterStemmer algorithm\n",
    "    words = [nltk.stem.PorterStemmer().stem(word) for word in words]\n",
    "\n",
    "    # remove stopwords\n",
    "    words = [word for word in words if not word in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tokenized_description\"] = train[\"description\"].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train, open(f\"../dump/train_{feature}.pkl\", \"wb\"))\n",
    "pickle.dump(\n",
    "    test,\n",
    "    open(f\"../dump/test_{feature}.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(\n",
    "    open(f\"../dump/train_{feature}.pkl\", \"rb\")\n",
    ")\n",
    "test = pickle.load(\n",
    "    open(f\"../dump/test_{feature}.pkl\", \"rb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(tokens):\n",
    "    return set([t for d in tokens for t in d])\n",
    "\n",
    "\n",
    "def get_term_frequency(tokens):\n",
    "    termFrequency = {}\n",
    "    for t in tokens:\n",
    "        termFrequency[t] = termFrequency.get(t, 0) + 1\n",
    "    return termFrequency\n",
    "\n",
    "\n",
    "def get_document_frequencies(termFrequencies):\n",
    "    documentFrequencies = {}\n",
    "    for documentTerms in termFrequencies:\n",
    "        for term in documentTerms:\n",
    "            documentFrequencies[term] = documentFrequencies.get(term, 0) + 1\n",
    "    return documentFrequencies\n",
    "\n",
    "\n",
    "def get_document_term_presence(documentTokens):\n",
    "    return [set(tokens) for tokens in documentTokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17641"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = get_vocabulary(train[\"tokenized_description\"])\n",
    "document_term_frequencies = list(\n",
    "    map(get_term_frequency, train[\"tokenized_description\"])\n",
    ")\n",
    "document_frequencies = get_document_frequencies(document_term_frequencies)\n",
    "document_presence = get_document_term_presence(train[\"tokenized_description\"])\n",
    "collection_dictionary = [(t, f) for t, f in sorted(document_frequencies.items())]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bergamot',\n",
       " 'Violet',\n",
       " 'Geranium',\n",
       " 'Pink Pepper',\n",
       " 'Benzoin',\n",
       " 'Grapefruit',\n",
       " 'Mandarin',\n",
       " 'Tonka Bean',\n",
       " 'Lily Of The Valley',\n",
       " 'Musk',\n",
       " 'Vetiver',\n",
       " 'Leather',\n",
       " 'Amber',\n",
       " 'Neroli',\n",
       " 'Incense - Olibanum',\n",
       " 'Sandalwood',\n",
       " 'Patchouli',\n",
       " 'Oud - Agarwood',\n",
       " 'Lemon',\n",
       " 'Ciste Labdanum',\n",
       " 'Orange Blossom',\n",
       " 'Cedarwood',\n",
       " 'Jasmine',\n",
       " 'Vanilla',\n",
       " 'Rose',\n",
       " 'Ylang-ylang',\n",
       " 'Lavender',\n",
       " 'Cardamom',\n",
       " 'Woody Notes',\n",
       " 'Iris - Orris']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = list(set(train[feature].values))\n",
    "class_frequencies = train[feature].value_counts()\n",
    "\n",
    "num_documents = len(train[feature])\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate\n",
    "# def expected_freq(term):\n",
    "#     document_frequency = len(\n",
    "#         [\n",
    "#             documentId\n",
    "#             for documentId in train['index']\n",
    "#             if term in document_presence[documentId]\n",
    "#         ]\n",
    "#     )\n",
    "#     return [\n",
    "#         document_frequency * f / num_documents for f in class_frequencies\n",
    "#     ], document_frequency\n",
    "\n",
    "\n",
    "# def observed_freq(term):\n",
    "#     return [\n",
    "#         len(train.loc[(train[feature] == class_name) & (term in train[\"tokenized_description\"])])\n",
    "#         for class_name in classes\n",
    "#     ]\n",
    "\n",
    "\n",
    "# def chi2(term):\n",
    "#     es, document_frequency = expected_freq(term)\n",
    "#     ns = observed_freq(term)\n",
    "#     if document_frequency == 0:\n",
    "#         return 0\n",
    "#     value = sum([((n - e) ** 2) / e for (n, e) in zip(ns, es)])\n",
    "#     return value\n",
    "\n",
    "\n",
    "# chi2s = [(chi2(term), term) for term in tqdm(vocabulary)]\n",
    "\n",
    "# chi2s_pq = DualPriorityQueue(maxPQ=True)\n",
    "# [chi2s_pq.put(chi2, term) for (chi2, term) in chi2s]\n",
    "\n",
    "# chi2_sorted_terms = []\n",
    "# while not chi2s_pq.empty():\n",
    "#     chi2_sorted_terms.append(chi2s_pq.get())\n",
    "\n",
    "# json.dump(chi2_sorted_terms, open(f\"../dump/chi2_sorted_terms_{feature}.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from dump\n",
    "chi2_sorted_terms = json.load(open(f\"../dump/chi2_sorted_terms_{features[0]}.json\", \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_p(class_name, selected_terms):\n",
    "    class_documents = train.loc[(train[feature] == class_name)]\n",
    "    prior = len(class_documents) / num_documents\n",
    "    class_document_tokens = []\n",
    "\n",
    "    for document in class_documents[\"tokenized_description\"]:\n",
    "        tokens = document\n",
    "        for token in tokens:\n",
    "            if token in selected_terms:\n",
    "                class_document_tokens.append(token)\n",
    "    class_term_frequencies = {}\n",
    "    for token in class_document_tokens:\n",
    "        class_term_frequencies[token] = class_term_frequencies.get(token, 0) + 1\n",
    "    token_count_sum = len(class_document_tokens) + len(selected_terms)\n",
    "    cond_prob = {\n",
    "        term: ((class_term_frequencies.get(term, 0) + 1) / token_count_sum) # smoothing\n",
    "        for term in selected_terms\n",
    "    }\n",
    "    return {\"prior\": prior, \"cond_prob\": cond_prob}\n",
    "\n",
    "def calculate_classes_p(selected_terms):\n",
    "  class_p = {\n",
    "      class_name: calculate_class_p(class_name, selected_terms)\n",
    "      for class_name in classes\n",
    "  }\n",
    "\n",
    "  return class_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_options = [100, 200, 400, 1000, 2000, 4000, 10000]\n",
    "for i in top_k_options:\n",
    "    selected_terms = set([term for _, term in chi2_sorted_terms[:i]])\n",
    "    classes_p = calculate_classes_p(selected_terms)\n",
    "    json.dump({\"vocabulary\": list(selected_terms), \"p\": classes_p}, open(f\"../dump/class_p_{feature}_top_{i}.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def evaluate_classifier(test_documents, feature, multi_label=False, top_K=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    classifier = Classifier(feature, multi_label, top_K)\n",
    "\n",
    "    # Iterate through the test documents\n",
    "    for document, true_label in test_documents:\n",
    "        # Get the predicted class (or classes) for the document\n",
    "        predicted_label = classifier.classification(document)\n",
    "\n",
    "        # If multi_label is True and classifier returns multiple labels\n",
    "        if multi_label:\n",
    "            # If predicted_label is a list or set, handle as multi-label classification\n",
    "            if isinstance(predicted_label, (list, set)):\n",
    "                y_pred.append(predicted_label)  # Multiple predicted labels\n",
    "                y_true.append([true_label])  # Single true label wrapped in a list\n",
    "            else:\n",
    "                y_pred.append([predicted_label])  # Single predicted label\n",
    "                y_true.append([true_label])  # Single true label\n",
    "        else:\n",
    "            # For single class prediction\n",
    "            y_pred.append([predicted_label])\n",
    "            y_true.append([true_label])\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision, recall, f1 = calculate_precision_recall_f1(y_true, y_pred)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to calculate precision, recall, and F1 for each class (weighted average)\n",
    "def calculate_precision_recall_f1(y_true, y_pred):\n",
    "    # Initialize counters for true positives, false positives, and false negatives\n",
    "    true_positives = Counter()\n",
    "    false_positives = Counter()\n",
    "    false_negatives = Counter()\n",
    "    label_counts = Counter()\n",
    "\n",
    "    # Collect counts for precision and recall calculation\n",
    "    for true_labels, pred_labels in zip(y_true, y_pred):\n",
    "        for label in true_labels:\n",
    "            label_counts[label] += 1\n",
    "        for label in pred_labels:\n",
    "            if label in true_labels:\n",
    "                true_positives[label] += 1\n",
    "            else:\n",
    "                false_positives[label] += 1\n",
    "        for label in true_labels:\n",
    "            if label not in pred_labels:\n",
    "                false_negatives[label] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 for each label\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "\n",
    "    for label in label_counts:\n",
    "        tp = true_positives[label]\n",
    "        fp = false_positives[label]\n",
    "        fn = false_negatives[label]\n",
    "        total = label_counts[label]\n",
    "\n",
    "        # Calculate precision, recall, and F1 for this label\n",
    "        precision[label] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall[label] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1[label] = (\n",
    "            2 * (precision[label] * recall[label]) / (precision[label] + recall[label])\n",
    "            if (precision[label] + recall[label]) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    # Compute weighted averages based on the frequency of labels in y_true\n",
    "    total_labels = sum(label_counts.values())\n",
    "\n",
    "    weighted_precision = sum(\n",
    "        precision[label] * label_counts[label] / total_labels for label in precision\n",
    "    )\n",
    "    weighted_recall = sum(\n",
    "        recall[label] * label_counts[label] / total_labels for label in recall\n",
    "    )\n",
    "    weighted_f1 = sum(f1[label] * label_counts[label] / total_labels for label in f1)\n",
    "\n",
    "    return weighted_precision, weighted_recall, weighted_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Results for family =======\n",
      "=== Results for top 100 features === \n",
      "Precision: 0.43379\n",
      "Recall: 0.26236\n",
      "F1-Score: 0.28845\n",
      "=== Results for top 200 features === \n",
      "Precision: 0.43266\n",
      "Recall: 0.28517\n",
      "F1-Score: 0.30094\n",
      "=== Results for top 400 features === \n",
      "Precision: 0.4539\n",
      "Recall: 0.32319\n",
      "F1-Score: 0.33502\n",
      "=== Results for top 1000 features === \n",
      "Precision: 0.45776\n",
      "Recall: 0.3384\n",
      "F1-Score: 0.3528\n",
      "=== Results for top 2000 features === \n",
      "Precision: 0.44994\n",
      "Recall: 0.3384\n",
      "F1-Score: 0.3553\n",
      "=== Results for top 4000 features === \n",
      "Precision: 0.46361\n",
      "Recall: 0.39544\n",
      "F1-Score: 0.41061\n",
      "=== Results for top 10000 features === \n",
      "Precision: 0.46661\n",
      "Recall: 0.42205\n",
      "F1-Score: 0.43086\n",
      "======= Results for subfamily =======\n",
      "=== Results for top 100 features === \n",
      "Precision: 0.42782\n",
      "Recall: 0.14068\n",
      "F1-Score: 0.16112\n",
      "=== Results for top 200 features === \n",
      "Precision: 0.48716\n",
      "Recall: 0.1597\n",
      "F1-Score: 0.18405\n",
      "=== Results for top 400 features === \n",
      "Precision: 0.38817\n",
      "Recall: 0.1711\n",
      "F1-Score: 0.18384\n",
      "=== Results for top 1000 features === \n",
      "Precision: 0.37424\n",
      "Recall: 0.21673\n",
      "F1-Score: 0.23562\n",
      "=== Results for top 2000 features === \n",
      "Precision: 0.3604\n",
      "Recall: 0.23954\n",
      "F1-Score: 0.25836\n",
      "=== Results for top 4000 features === \n",
      "Precision: 0.35816\n",
      "Recall: 0.26236\n",
      "F1-Score: 0.27463\n",
      "=== Results for top 10000 features === \n",
      "Precision: 0.3301\n",
      "Recall: 0.25095\n",
      "F1-Score: 0.26611\n",
      "======= Results for ingredients =======\n",
      "=== Results for top 100 features === \n",
      "Precision: 0.038006\n",
      "Recall: 0.27074\n",
      "F1-Score: 0.063099\n",
      "=== Results for top 200 features === \n",
      "Precision: 0.040717\n",
      "Recall: 0.26697\n",
      "F1-Score: 0.065829\n",
      "=== Results for top 400 features === \n",
      "Precision: 0.041643\n",
      "Recall: 0.23379\n",
      "F1-Score: 0.064739\n",
      "=== Results for top 1000 features === \n",
      "Precision: 0.032474\n",
      "Recall: 0.16063\n",
      "F1-Score: 0.046087\n",
      "=== Results for top 2000 features === \n",
      "Precision: 0.034839\n",
      "Recall: 0.14027\n",
      "F1-Score: 0.04368\n",
      "=== Results for top 4000 features === \n",
      "Precision: 0.037084\n",
      "Recall: 0.11689\n",
      "F1-Score: 0.038554\n",
      "=== Results for top 10000 features === \n",
      "Precision: 0.040944\n",
      "Recall: 0.11538\n",
      "F1-Score: 0.040682\n",
      "======= Results for gender =======\n",
      "=== Results for top 100 features === \n",
      "Precision: 0.56212\n",
      "Recall: 0.55894\n",
      "F1-Score: 0.55668\n",
      "=== Results for top 200 features === \n",
      "Precision: 0.60225\n",
      "Recall: 0.60076\n",
      "F1-Score: 0.5992\n",
      "=== Results for top 400 features === \n",
      "Precision: 0.64008\n",
      "Recall: 0.63498\n",
      "F1-Score: 0.63458\n",
      "=== Results for top 1000 features === \n",
      "Precision: 0.66504\n",
      "Recall: 0.6616\n",
      "F1-Score: 0.66166\n",
      "=== Results for top 2000 features === \n",
      "Precision: 0.64517\n",
      "Recall: 0.64259\n",
      "F1-Score: 0.64177\n",
      "=== Results for top 4000 features === \n",
      "Precision: 0.66801\n",
      "Recall: 0.6654\n",
      "F1-Score: 0.66565\n",
      "=== Results for top 10000 features === \n",
      "Precision: 0.68475\n",
      "Recall: 0.68061\n",
      "F1-Score: 0.68083\n"
     ]
    }
   ],
   "source": [
    "feature_is_multilabel = {\n",
    "    'family': False,\n",
    "    'subfamily': False,\n",
    "    'ingredients': True,\n",
    "    'gender': False\n",
    "}\n",
    "\n",
    "for feature in features:\n",
    "  print(f\"======= Results for {feature} =======\")\n",
    "  train = pickle.load(\n",
    "    open(f\"../dump/train_{feature}.pkl\", \"rb\")\n",
    "  )\n",
    "  test = pickle.load(\n",
    "      open(f\"../dump/test_{feature}.pkl\", \"rb\")\n",
    "  )\n",
    "  for k in top_k_options:\n",
    "      print(f'=== Results for top {k} features === ')\n",
    "      precision, recall, f1 = evaluate_classifier(\n",
    "          zip(test[\"description\"], test[feature]),\n",
    "          feature,\n",
    "          multi_label=feature_is_multilabel[feature],\n",
    "          top_K=k,\n",
    "      )\n",
    "      print(f\"Precision: {precision:.5}\")\n",
    "      print(f\"Recall: {recall:.5}\")\n",
    "      print(f\"F1-Score: {f1:.5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fragrance-data-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
